{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations Prediction Pipeline\n",
    "\n",
    "**Author**: [Your Name]\n",
    "\n",
    "**Date**: [Date]\n",
    "\n",
    "**Pipeline ID**: `arxiv_citations_v1` (or your chosen ID)\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "This notebook implements a complete pipeline for generating citation prediction questions from arXiv papers.\n",
    "\n",
    "**High-level approach**:\n",
    "- [Describe your approach here]\n",
    "- [arXiv categories selected and why]\n",
    "- [Paper pairing strategy]\n",
    "- [Citation data source]\n",
    "\n",
    "**Key design decisions**:\n",
    "- [Major decision 1 and rationale]\n",
    "- [Major decision 2 and rationale]\n",
    "- [etc.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from src.data_classes import ForecastingQuestion, ArxivPaper\n",
    "\n",
    "# TODO: Add your additional imports for scraping, citation APIs, etc.\n",
    "# Example: import arxiv, requests, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Describe your data collection approach:\n",
    "- Which arXiv categories?\n",
    "- What time period?\n",
    "- How many papers collected?\n",
    "- Any initial filtering?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement arXiv scraping\n",
    "# Your code here to:\n",
    "# 1. Query arXiv API for papers\n",
    "# 2. Extract full paper text\n",
    "# 3. Get publication dates\n",
    "# 4. Save to JSONL format\n",
    "\n",
    "# Example structure:\n",
    "# papers = scrape_arxiv_papers(\n",
    "#     categories=['cs.LG', 'cs.AI'],\n",
    "#     start_date='2025-04-01',\n",
    "#     end_date='2025-05-01'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Citation Data Collection\n",
    "\n",
    "Describe how you obtained citation counts:\n",
    "- Data source (Semantic Scholar, Google Scholar, etc.)\n",
    "- Collection timestamp\n",
    "- Any API limitations or challenges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement citation collection\n",
    "# Your code here to:\n",
    "# 1. Query citation API\n",
    "# 2. Match papers to citation counts\n",
    "# 3. Add citation data to ArxivPaper objects\n",
    "\n",
    "# Example:\n",
    "# papers_with_citations = add_citation_counts(\n",
    "#     papers\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Quality Checks\n\nShow validation of your data:\n- Distribution of citation counts\n- Publication date distribution\n- Paper length statistics\n- Category distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add data validation and visualization\n",
    "# Show distributions, check for outliers, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Paper Pairing\n",
    "\n",
    "Describe your pairing strategy:\n",
    "- Minimum citation difference chosen and why\n",
    "- Maximum publication date gap\n",
    "- Category matching approach\n",
    "- How you avoid spurious cues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Implement pair_papers function\n# Write a function that takes a list of papers and returns a list of pairs of papers.\n# Think carefully about spurious cues and how to create a high-quality evaluation dataset.\n\n# Example signature:\n# def pair_papers(papers: List[ArxivPaper], ...) -> List[Tuple[ArxivPaper, ArxivPaper]]:\n#     \"\"\"\n#     Create pairs of papers for citation comparison.\n#     \"\"\"\n#     pass\n\n# pairs = pair_papers(papers_with_citations)\n# print(f\"Created {len(pairs)} pairs from {len(papers_with_citations)} papers\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Question Generation\n",
    "\n",
    "Generate questions from pairs using the standard template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Implement make_citations_comparison_question function\n# This function should take two ArxivPaper objects and return a ForecastingQuestion\n# Refer to src/data_classes.py for the ForecastingQuestion schema\n\n# Template for the question text:\nARXIV_CITATION_COMPARISON_PROMPT = \"\"\"\nWill paper A receive more citations than paper B by {paper_a_citation_timestamp}? Yes or No? Here are the titles, abstracts, text and publication dates for both papers.\n\n<paper_a>\n<title>{paper_a_title}</title>\n<full_text>{paper_a_full_text}</full_text>\n<publication_date>{paper_a_published_timestamp}</publication_date>\n</paper_a>\n\n<paper_b>\n<title>{paper_b_title}</title>\n<full_text>{paper_b_full_text}</full_text>\n<publication_date>{paper_b_published_timestamp}</publication_date>\n</paper_b>\n\nResolution Criteria:\nThis question resolves to \"Yes\" if Paper A has more citations than Paper B on {paper_a_citation_timestamp}.\nThis question resolves to \"No\" if Paper B has more citations than Paper A on {paper_a_citation_timestamp}.\n\nQuestion: Will paper A have more citations on {paper_a_citation_timestamp}, than paper B, Yes or No?\"\"\"\n\n# def make_citations_comparison_question(paper_a: ArxivPaper, paper_b: ArxivPaper) -> ForecastingQuestion:\n#     \"\"\"\n#     Create a ForecastingQuestion comparing citation counts of two papers.\n#     Use the ARXIV_CITATION_COMPARISON_PROMPT template above and populate all required ForecastingQuestion fields.\n#     \"\"\"\n#     pass\n\n# Generate questions from pairs\n# questions = []\n# for paper_a, paper_b in pairs:\n#     q = make_citations_comparison_question(paper_a, paper_b)\n#     questions.append(q)\n# \n# print(f\"Generated {len(questions)} questions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Mistakes Analysis\n",
    "\n",
    "For each issue in the worktest document, address:\n",
    "\n",
    "### 6.1 Spurious Cues\n",
    "- **Issue**: [Describe the issue]\n",
    "- **Your dataset**: [Yes/No - does your dataset have this issue?]\n",
    "- **Mitigation**: [What you did to address it]\n",
    "- **Tradeoffs**: [What you gave up]\n",
    "\n",
    "### 6.2 Ambiguity in Questions\n",
    "- **Issue**: [Describe]\n",
    "- **Your dataset**: [Analysis]\n",
    "- **Mitigation**: [Steps taken]\n",
    "- **Tradeoffs**: [Costs]\n",
    "\n",
    "### 6.3 Low Signal-to-Noise Ratio\n",
    "- **Issue**: [Describe]\n",
    "- **Your dataset**: [Analysis]\n",
    "- **Mitigation**: [Steps taken]\n",
    "- **Tradeoffs**: [Costs]\n",
    "\n",
    "### 6.4 Selection Effects and Biases\n",
    "- **Issue**: [Describe]\n",
    "- **Your dataset**: [Analysis]\n",
    "- **Mitigation**: [Steps taken]\n",
    "- **Tradeoffs**: [Costs]\n",
    "\n",
    "### 6.5 Data Contamination\n",
    "- **Issue**: [Describe]\n",
    "- **Your dataset**: [Analysis]\n",
    "- **Mitigation**: [Steps taken]\n",
    "- **Tradeoffs**: [Costs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "#Any code you might want to write to check and rectify the issues mentioned above before running the evaluation. \n",
    "# Are there paper pairs that dont make sense? Is there something wrong with the way you're collecting citations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Question Evaluation\n",
    "\n",
    "Evaluate your dataset with multiple Claude models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import evaluate_and_plot\n",
    "import asyncio\n",
    "\n",
    "async def run_evaluation():\n",
    "    predictions, metrics = await evaluate_and_plot(\n",
    "        questions[100],\n",
    "        model_ids=[\n",
    "            \"claude-3-5-haiku-latest\",\n",
    "            \"claude-3-7-sonnet-20250219\",\n",
    "            \"claude-sonnet-4-20250514\"\n",
    "        ],\n",
    "        output_dir=Path(\"evaluation_results\"),\n",
    "        experiment_name=\"citations_pipeline\"\n",
    "    )\n",
    "    return predictions, metrics\n",
    "\n",
    "# Run evaluation\n",
    "predictions, metrics = asyncio.run(run_evaluation())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis\n",
    "- Are you seeing accuracy scaling with more intelligent models? \n",
    "- If not, try to reason why not. Are there any spurious cues you need to remove? Is your datapipeline broken? Are there papers that are skewing your results? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "#Any code you might want to write to debug your pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}